% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/IBM4.R
\name{IBM4}
\alias{IBM4}
\title{IBM4 Model}
\usage{
IBM4(
  e,
  e_wordclass,
  f,
  f_wordclass,
  maxiter = 5,
  eps = 0.01,
  heuristic = TRUE,
  maxfert = 5,
  init.IBM1 = 5,
  init.IBM2 = 5,
  init.IBM3 = 3,
  sparse = FALSE,
  fmatch = FALSE
)
}
\arguments{
\item{e}{vector of sentences in language we want to translate to}

\item{e_wordclass}{named vector where names are each unique e word and values are the "class" of each e word numbered from 1 to n_e_class where n_e_class is the total number of word classes.}

\item{f}{vector of sentences in language we want to translate from}

\item{f_wordclass}{named vector where names are each unique f word and values are the "class" of each f word numbered from 1 to n_f_class where n_f_class is the total number of word classes.}

\item{maxiter}{max number of EM iterations allowed (default 5)}

\item{eps}{convergence criteria for perplexity (i.e. negative log-likelihood)}

\item{heuristic}{If TRUE (default) use a heuristic hill-climbing algorithm to find most likely alignments. If FALSE, search over all alignments (not recommended unless only looking at small sentences.)}

\item{maxfert}{Maximum number of e words ("fertility") which an f word is allowed to be mapped to. The default is 5. In practice fertility tends to be small, so this number should normally be sufficient.}

\item{init.IBM1}{number of iterations (integer>=0) of IBM1 to perform to initialize IBM2 algorithm (default 5)}

\item{init.IBM2}{number of iterations (integer>=0) of IBM2 to perform to initialize IBM3 algorithm (default 5)}

\item{init.IBM3}{number of iterations (integer>=0) of IBM3 to perform to initialize IBM4 algorithm (default 3)}

\item{sparse}{If TRUE, uses sparse matrices from Matrix package (default FALSE).}
}
\value{
\item{tmatrix}{Matrix of translation probabilities (cols are words from e, rows are words from f). If sparse=TRUE, tmatrix will be a sparseMatrix from the Matrix package, and will generally take up substantially less memory.}
\item{d1}{Array of relative distortion probabilities for the first word in a cept.}
\item{dg1}{Array of relative distortion probabilities for subsequent words in a cept.}
\item{fertmatrix}{Matrix of fertility probabilities.}
\item{p_null}{Probability of NULL token insertion.}
\item{numiter}{Number of iterations}
\item{maxiter}{As above}
\item{eps}{As above}
\item{converged}{TRUE if algorithm stopped once eps criteria met. FALSE otherwise.}
\item{perplexity}{Final likelihood/perplexity value.}
\item{time_elapsed}{Time in minutes the algorithm ran for.}
}
\description{
The fourth SMT model from Brown et al. (1993)
}
\examples{
# download english-french sentence pairs
temp = tempfile()
download.file("http://www.manythings.org/anki/fra-eng.zip",temp);
ENFR = readr::read_tsv(file=unz(temp,"fra.txt"),col_names=c("en","fr","details"));
unlink(temp);

# a bit of pre-processing
e = tolower(stringr::str_squish(tm::removePunctuation(ENFR$en[1:200])));
f = tolower(stringr::str_squish(tm::removePunctuation(ENFR$fr[1:200])));
e_all = unique(unlist(stringr::str_split(e, pattern=" ")))
f_all = unique(unlist(stringr::str_split(f, pattern=" ")))

# get POS tagging models from udpipe package
m_eng = udpipe::udpipe_download_model(language = "english-ewt")
m_eng = udpipe::udpipe_load_model(m_eng)
m_fr = udpipe::udpipe_download_model(language = "french-gsd")
m_fr = udpipe::udpipe_load_model(m_fr)

# annotate e and f words
en_annotate = udpipe::udpipe_annotate(m_eng, x = e_all) \%>\%
  as.data.frame() \%>\%
  dplyr::select(-sentence)
fr_annotate = udpipe::udpipe_annotate(m_fr, x = f_all) \%>\%
  as.data.frame() \%>\%
  dplyr::select(-sentence)

# code nouns as 1, everything else (including missings) as 2
ecl = as.numeric(en_annotate$upos=="NOUN")+1
  names(ecl) = e_all
  ecl[is.na(ecl)] = 2
fcl = as.numeric(fr_annotate$upos=="NOUN")+1
  names(fcl) = f_all
  fcl[is.na(fcl)] = 2

# run model
model4 = IBM4(
  e=e, e_wordclass=ecl, f=f, f_wordclass=fcl,
  maxiter=10, init.IBM1=10, init.IBM2=20, init.IBM3=10
)

}
