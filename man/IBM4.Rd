% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/IBM4.R
\name{IBM4}
\alias{IBM4}
\title{IBM4 Model}
\usage{
IBM4(
  target,
  source,
  target_wordclass = NULL,
  source_wordclass = NULL,
  maxiter = 30,
  eps = 0.01,
  heuristic = TRUE,
  maxfert = 5,
  init.IBM1 = 10,
  init.IBM2 = 10,
  init.IBM3 = 10,
  init.tmatrix = NULL,
  init.amatrix = NULL,
  init.fmatrix = NULL,
  init.dmatrix = NULL,
  init.d1array = NULL,
  init.dg1array = NULL,
  init.p_null = NULL,
  verbose = 0.5
)
}
\arguments{
\item{target}{vector of sentences in language we want to translate to. Function assumes sentences are space-delimited.}

\item{source}{vector of sentences in language we want to translate from. Function assumes sentences are space-delimited.}

\item{target_wordclass}{named vector where names are each unique target word and values are the "class" of each target word numbered from 1 to n_target_class where n_target_class is the total number of target word classes. If not provided, assumes all words in the same class.}

\item{source_wordclass}{named vector where names are each unique source word and values are the "class" of each source word numbered from 1 to n_source_class where n_source_class is the total number of source word classes. (do NOT include \if{html}{\out{<NULL>}} token). If not provided, assumes all words in the same class.}

\item{maxiter}{max number of EM iterations allowed}

\item{eps}{convergence criteria for perplexity (i.e. negative log-likelihood)}

\item{heuristic}{If TRUE (default) use a heuristic hill-climbing algorithm to find most likely alignments. If FALSE, search over all alignments (not recommended unless only looking at short sentences). Sentences that are length 3 or smaller with always search over all alignments, even if heuristic=TRUE.}

\item{maxfert}{Maximum number of e words ("fertility") to which an f word can be aligned. The default is 5.}

\item{init.IBM1}{number of iterations (integer>=0) of IBM1 to perform to initialize IBM2 algorithm}

\item{init.IBM2}{number of iterations (integer>=0) of IBM2 to perform to initialize IBM3 algorithm}

\item{init.IBM3}{number of iterations (integer>=0) of IBM3 to perform to initialize IBM4 algorithm}

\item{init.tmatrix}{tmatrix from a previous estimation. Used to initialize IBM1 (if \code{init.IBM1}>0), IBM2 (if \code{init.IBM2} >0), or IBM3 otherwise. If not provided, algorithm starts with uniform probabilities.}

\item{init.amatrix}{amatrix from a previous estimation. Used to initialize IBM2 if \code{init.IBM2} >0. If not provided, algorithm starts with uniform probabilities.}

\item{init.fmatrix}{fmatrix from a previous estimation. Used to initialize IBM3. If not provided, algorithm starts with uniform probabilities.}

\item{init.dmatrix}{dmatrix from a previous estimation. Used to initialize IBM3. If not provided, algorithm starts with uniform probabilities.}

\item{init.d1array}{d1array from a previous estimation. Used to initialize IBM4. If not provided, algorithm starts with uniform probabilities.}

\item{init.dg1array}{dg1array from a previous estimation. Used to initialize IBM4. If not provided, algorithm starts with uniform probabilities.}

\item{init.p_null}{p_null  from a previous estimation. Used to initialize IBM3. If not provided, algorithm starts with \code{p_null=0.5}.}

\item{verbose}{If >=1, shows progress bar which updates every \code{verbose} steps, plus a summary when each iteration is complete. If 0.5 (default), only shows the summary without progress bars. If 0, shows nothing.}
}
\value{
\item{tmatrix}{Environment object containing translation probabilities for target-source word pairs. E.g. tmatrix$go$va (equivalently, tmatrix[\link{"go"}][\link{"va"}]) gives the probability of target="go" given source="va".}
\item{d1array}{An array}
\item{dg1array}{An array}
\item{fmatrix}{Environment object containing vectors of fertility probabilities for each source word. E.g. fmatrix[\link{"va"}][1] is probability that "va" is aligned with 0 words. fmatrix[\link{"va"}][3] is probability "va" is aligned with 2 words. Each vector is length \code{maxfert+1}.}
\item{p_null}{Probability of NULL token insertion.}
\item{numiter}{Number of iterations}
\item{maxiter}{As above}
\item{eps}{As above}
\item{converged}{TRUE if algorithm stopped once eps criteria met. FALSE otherwise.}
\item{perplexity}{Final likelihood/perplexity value.}
\item{time_elapsed}{Time in minutes the algorithm ran for.}
\item{corpus}{data frame containing the target and source sentences and their lengths}
\item{best_alignments}{list containing best alignments (i.e. "viterbi" alignments) for each target sentence}
}
\description{
The fourth SMT model from Brown et al. (1993). Note that, unlike IBM1 and IBM2 functions,
NULL insertion is always assumed for IBM3 and 4, and cannot be turned off. Note also that I use
the basic hill climbing algorithm, and do not use pegging to increase the number
of alignments considered.
}
\examples{
# download english-french sentence pairs
temp = tempfile()
download.file("http://www.manythings.org/anki/fra-eng.zip",temp);
ENFR = readr::read_tsv(file=unz(temp,"fra.txt"),col_names=c("en","fr","details"));
unlink(temp);

# a bit of pre-processing
e = tolower(stringr::str_squish(tm::removePunctuation(ENFR$en[1:10000])));
f = tolower(stringr::str_squish(tm::removePunctuation(ENFR$fr[1:10000])));

# don't use any word classes, i.e. word class is 1 for all words
all_e_words = unique(unlist(lapply(X=e,FUN=function(s) unlist(stringr::str_split(s, " ")))))
all_f_words = unique(unlist(lapply(X=f,FUN=function(s) unlist(stringr::str_split(s, " ")))))
class1 = rep(1,length(all_e_words)); names(class1)=all_e_words
class2 = rep(1,length(all_f_words)); names(class2)=all_f_words

# a model
model4 = IBM4(target=e,target_wordclass=class1,
                source=f,source_wordclass=class2,
                maxiter=5, init.IBM1=10, init.IBM2=10, init.IBM3=2,
                verbose=100)

}
