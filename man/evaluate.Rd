% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate.R
\name{evaluate}
\alias{evaluate}
\title{Evaluate Translation}
\usage{
evaluate(
  hypothesis,
  reference,
  type = c("precision", "recall", "f-measure", "PER", "WER", "BLEU"),
  BLEU.maxorder = 4,
  BLEU.weights = rep(1/BLEU.maxorder, BLEU.maxorder)
)
}
\arguments{
\item{hypothesis}{the translation we wish to evaluate}

\item{reference}{the "ground truth" translation against which we compare our hypothesis}

\item{type}{character vector containing any of "precision","recall","f-measure","PER","WER","BLEU" corresponding to each type of evaluation metric. Default is all of them.}

\item{BLEU.maxorder}{If "BLEU" specified, this is the maximum ngram precision to consider in the calculation. (integer >= 1)}

\item{BLEU.weights}{If "BLEU" specified, this is a numeric vector of weights applied to the ngram precisions. Thus, its length should be equal to \code{BLEU.maxorder}. If sum of weights don't equal 1, they are rescaled.}
}
\value{
A list containing the values of each evaluation metric. "BLEU" contains a sublist with information about the assumptions for the BLEU calculation.
}
\description{
Calculates a series of evaluation metrics for a hypothesis translation based
on a given reference translation. Based on Koehn's "Statistical Machine Translation"
Chapter 8. Word Error Rate (WER) adapted from https://www.thepythoncode.com/article/calculate-word-error-rate-in-python
}
\examples{
systemA = "Israeli officials responsibility of airport safety"
systemB = "airport security Israeli officials are responsible"
reference   = "Israeli officials are responsible for airport security"
evaluate(systemA,reference)
evaluate(systemB,reference)
}
