% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/decode_phrase_table.R
\name{decode.phrase_table}
\alias{decode.phrase_table}
\title{Phrase-based translation stack decoder.}
\usage{
\method{decode}{phrase_table}(
  object,
  target.sentence,
  max_stack_size = 100,
  senlength.model = NULL,
  language.model = NULL
)
}
\arguments{
\item{object}{result from build_phrase_table()}

\item{target.sentence}{sentence in f language we'd like to decode (character string)}

\item{max_stack_size}{(default 100) the maximum number of partial translations to keep from each stack. Lower number means faster algorithm, but more likely best translation is missed.}

\item{senlength.model}{Matrix such that \code{senlength.model[m,n]} is the probability that e sentence is length m given f sentence is length n. If not provided, it is estimated form the training corpus using a simple poisson regression: \code{glm(e_lengths ~ f_lengths, family="poisson")}.}

\item{language.model}{The result of a \code{kgrams::language_model} estimation. If not provided, we estimate a degree-3 kgrams model from the training corpus using the default "ml" (maximum likelihood) approach. See \code{kgrams} package for more details.}
}
\value{
List of best translations found using the stack decoder.
}
\description{
Implements the stack decoder described in \code{Koehn (2009)} with help from the NLTK (python package) stack decoder code.
One key difference is that I don't currently implement future cost estimation,
which can allow better ranking of partial hypotheses.
Relies on three models: phrase translation table, sentence length model,
and a kgrams language model. If the latter two aren't provided, they are estimated
using a similar approach to \verb{Wang and Waibel (1998)}: sentence length is estimated
using a poisson regression, and the language model is estimated using 3rd-order kgrams.
Heuristics (\code{max_stack_size}) are used to reduce computational time,
at the expense of an increased possibility of not finding the best translation.
}
\examples{
# download english-french sentence pairs
temp = tempfile()
download.file("http://www.manythings.org/anki/fra-eng.zip",temp);
ENFR = readr::read_tsv(file=unz(temp,"fra.txt"),col_names=c("en","fr","details"));
unlink(temp);

# a bit of pre-processing
e = removePunctuation(ENFR$en[1:10000])
  e = str_squish(e)
  e = tolower(e)
f = removePunctuation(ENFR$fr[1:10000])
  f = str_squish(f)
  f = tolower(f)

# estimate models in both directions
model3_etof = IBM3(target=e,source=f,maxiter=5, init.IBM1=15, init.IBM2=15,  verbose=100)
model3_ftoe = IBM3(target=f,source=e,maxiter=5, init.IBM1=15, init.IBM2=15,  verbose=100)

# extract phrases from the IBM models and compute phrase probabilities
phtable = build_phrase_table(model3_etof,model3_ftoe)
# possible english translations and their probabilities
best_translation = decode(phtable,target.sentence = phtable$corpus$target[[6000]]) # "laissemoi réfléchir"
  # returns "let me think"

}
